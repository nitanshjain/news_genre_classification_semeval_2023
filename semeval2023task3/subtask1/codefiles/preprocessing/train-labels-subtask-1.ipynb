{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code provided below can be used for train, test and dev data. Only the path of the files will need to be changed and certain blocks will not be require to run in case of test data. Translation of languages has been done using Google Translator."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nitanshjain/.local/share/virtualenvs/Sem_Eval-qQJYuRaW/lib/python3.10/site-packages/spacy/util.py:877: UserWarning: [W095] Model 'en_core_web_sm' (3.4.1) was trained with spaCy v3.4 and may not be 100% compatible with the current version (3.5.0). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import regex as re\n",
    "import spacy as sy\n",
    "import string\n",
    "from urllib.parse import urlparse\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "\n",
    "nlp_en = sy.load('en_core_web_sm')\n",
    "all_stopwords = nlp_en.Defaults.stop_words\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Labels from train-labels-subtask-1.txt and News Articles from train-articles-subtask-1 into separate dataframes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Languages are provided so we can shift between loading data of multiple languages by changing the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and reading train-labels-subtask-1.txt\n",
    "filename = '/Users/nitanshjain/Documents/Projects/Sem_Eval/semeval2023task3/data/en/train-labels-subtask-1.txt'\n",
    "train_sub1_df = pd.read_csv(filename, header=None, sep='\\t', names=['id', 'genre'])\n",
    "print(train_sub1_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading and reading all .txt files in train-articles-subtask-1\n",
    "dir_name = '/Users/nitanshjain/Documents/Projects/Sem_Eval/semeval2023task3/data/en/train-articles-subtask-1'\n",
    "\n",
    "article_df = pd.DataFrame()\n",
    "\n",
    "numbers = list()\n",
    "headlines = list()\n",
    "articles = list()\n",
    "\n",
    "for file in os.walk(dir_name):\n",
    "    for filename in file[2]:\n",
    "        \n",
    "        number = re.findall('[0-9]+', filename)\n",
    "        numbers.append(number[0])                       # appending the id to the list numbers\n",
    "        x = dir_name + '/' + filename\n",
    "        # print(number)\n",
    "        article = ''\n",
    "        with open(x) as f:\n",
    "            lines = f.readlines()\n",
    "            for i in range(len(lines)):\n",
    "                if i==0:\n",
    "                    headline = lines[0]\n",
    "                    headlines.append(headline)          # appending the headline of the news articles to the list headlines\n",
    "                elif lines[i]==\"\\n\":\n",
    "                    continue\n",
    "                else:\n",
    "                    # print()\n",
    "                    article = article + '' + lines[i]   \n",
    "        articles.append(article)                        # appending the main body of the new artcile to the list articles\n",
    "\n",
    "# creating a dataframe with the columns id, headline and article\n",
    "article_df['id'] = numbers                             \n",
    "article_df['headlines'] = headlines\n",
    "article_df['articles'] = articles\n",
    "\n",
    "# converting the id column to int32\n",
    "article_df = article_df.astype({'id': 'int32'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_sub1_df.set_index('id').join(article_df.set_index('id'))   # use for training and dev data\n",
    "# train_df = article_df                                                       # use for test data\n",
    "print(train_df.isnull().sum())                                              # checking for null values\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting count of labels in the genre column, works only for train and dev data\n",
    "print(train_df.genre.value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Translating Other Data from other languages to english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to translate the text to english\n",
    "# y - column name\n",
    "# lang - language of the text provided\n",
    "# df - dataframe\n",
    "\n",
    "def translation(y, lang, df):\n",
    "    translated_val = list()\n",
    "    \n",
    "    # translating the text to english using GoogleTranslator API\n",
    "    for x in df[y]:\n",
    "        try:\n",
    "            # GoogleTranslator API has a limit of 5000 characters per request, so splitting the text into chunks of 5000 characters and then translating it\n",
    "            if len(str(x))<5000:\n",
    "                \n",
    "                translation = GoogleTranslator(source=lang, target='en').translate(x)\n",
    "                translated_val.append(translation)\n",
    "            \n",
    "            elif len(str(x))>5000 and len(str(x))<10000:\n",
    "                \n",
    "                split_x = [x[i:i+4999] for i in range(0, len(x), 4999)]\n",
    "                translation_1 = GoogleTranslator(source=lang, target='en').translate(split_x[0])\n",
    "                translation_2 = GoogleTranslator(source=lang, target='en').translate(split_x[1])\n",
    "                translation = translation_1 + translation_2\n",
    "                translated_val.append(translation)\n",
    "                \n",
    "            elif len(str(x))>10000:\n",
    "                \n",
    "                split_x = [x[i:i+4999] for i in range(0, len(x), 4999)]\n",
    "                translation_1 = GoogleTranslator(source=lang, target='en').translate(split_x[0])\n",
    "                translation_2 = GoogleTranslator(source=lang, target='en').translate(split_x[1])\n",
    "                translation_3 = GoogleTranslator(source=lang, target='en').translate(split_x[2])\n",
    "                translation = translation_1 + translation_2 + translation_3\n",
    "                translated_val.append(translation)\n",
    "                \n",
    "        except Exception as e:\n",
    "            # if the text is not in the language provided, then it will return a nan value or text length is more than 15000 characters\n",
    "            translated_val.append(np.nan)\n",
    "    \n",
    "    # replacing the original text with the translated text\n",
    "    df[y] = translated_val\n",
    "    \n",
    "    # returning the updated dataframe\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# translating the headlines and articles to english and creating a new dataframe\n",
    "train_trans_df = translation('headlines', 'en', train_df)\n",
    "train_trans_df = translation('articles', 'en', train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "genre        0\n",
      "headlines    0\n",
      "articles     0\n",
      "dtype: int64\n",
      "genre        0\n",
      "headlines    0\n",
      "articles     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# dropping the rows with nan values\n",
    "print(train_trans_df.isna().sum())\n",
    "train_trans_df.dropna(inplace=True)\n",
    "print(train_trans_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>genre</th>\n",
       "      <th>headlines</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26136</th>\n",
       "      <td>opinion</td>\n",
       "      <td>The US is throwing its tentacles towards Russi...</td>\n",
       "      <td>“The Hand of the State Department”\\nThe influe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>opinion</td>\n",
       "      <td>The Draghistan War</td>\n",
       "      <td>In the newspeak of the Orwellian Atlanticist c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26248</th>\n",
       "      <td>opinion</td>\n",
       "      <td>\"Nazi speech worthy of Goebbels\", Orban advise...</td>\n",
       "      <td>Hegedus, a member of the Hungarian prime minis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26231</th>\n",
       "      <td>opinion</td>\n",
       "      <td>Landings and infections: the scourge of illega...</td>\n",
       "      <td>Taranto, 13 July - The arrival at the port of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26181</th>\n",
       "      <td>opinion</td>\n",
       "      <td>Super Mario's dilemma</td>\n",
       "      <td>When the time for the irresponsible strikes, p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         genre                                          headlines  \\\n",
       "id                                                                  \n",
       "26136  opinion  The US is throwing its tentacles towards Russi...   \n",
       "264    opinion                                 The Draghistan War   \n",
       "26248  opinion  \"Nazi speech worthy of Goebbels\", Orban advise...   \n",
       "26231  opinion  Landings and infections: the scourge of illega...   \n",
       "26181  opinion                              Super Mario's dilemma   \n",
       "\n",
       "                                                articles  \n",
       "id                                                        \n",
       "26136  “The Hand of the State Department”\\nThe influe...  \n",
       "264    In the newspeak of the Orwellian Atlanticist c...  \n",
       "26248  Hegedus, a member of the Hungarian prime minis...  \n",
       "26231  Taranto, 13 July - The arrival at the port of ...  \n",
       "26181  When the time for the irresponsible strikes, p...  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the Translated Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(x, y, df):\n",
    "    \n",
    "    pos_tags_final_text = list()\n",
    "    er_final_text = list()\n",
    "    preprocessed_text = list()\n",
    "\n",
    "    for x in df.loc[:,y]:\n",
    "\n",
    "        tokenizer = TweetTokenizer()\n",
    "        #tokenizing\n",
    "        doc = tokenizer.tokenize(x)\n",
    "        \n",
    "        # removing links\n",
    "        tokens = [token for token in doc if not urlparse(token).scheme]\n",
    "        x = ' '.join(tokens)\n",
    "        doc = nlp_en(x)\n",
    "        \n",
    "        # removing punctuation and white space\n",
    "        tokens = [token.orth_ for token in doc if not token.is_punct | token.is_space]    \n",
    "        x = ' '.join(tokens)\n",
    "        \n",
    "        # lower case\n",
    "        x = x.lower()\n",
    "        doc = nlp_en(x)\n",
    "\n",
    "        # lemmatization\n",
    "        tokens = [word.lemma_ for word in doc]   \n",
    "        x = ' '.join(tokens)\n",
    "        doc = nlp_en(x)  \n",
    "        \n",
    "        # removing punctuation and white space\n",
    "        tokens = [token.orth_ for token in doc if not token.is_punct | token.is_space]    \n",
    "        x = ' '.join(tokens)\n",
    "        doc = nlp_en(x)\n",
    "        \n",
    "        # removing individual letters\n",
    "        tokens = [word.text for word in doc if len(word)>=2]\n",
    "        x = ' '.join(tokens)  \n",
    "        # print(x)\n",
    "        doc = nlp_en(x)\n",
    "        \n",
    "        # removing stop words\n",
    "        tokens = [word for word in doc if not word in all_stopwords]\n",
    "        list_of_strings  = [i.text for i in tokens]\n",
    "        x = ' '.join(list_of_strings)\n",
    "        doc = nlp_en(x)\n",
    "        \n",
    "        # #removing numbers\n",
    "        x = ''.join([i for i in x if not i.isdigit()])\n",
    "        doc = nlp_en(x)\n",
    "        \n",
    "        # Part of speech tagging\n",
    "        pos_tags = [(i, i.tag_) for i in doc]\n",
    "        pos_tags_final_text.append(pos_tags)\n",
    "        \n",
    "        # entity recognition tagging\n",
    "        er =  [(i, i.label_, i.label) for i in doc.ents] \n",
    "        er_final_text.append(er)\n",
    "    \n",
    "        \n",
    "        preprocessed_text.append(x)\n",
    "        \n",
    "    # return list with pos tags, entity recognition and preprocessed text\n",
    "    return pos_tags_final_text, er_final_text, preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the headlines\n",
    "pos_tags_final_headlines, er_final_headlines, preprocessed_headlines = preprocessing(headline, 'headlines', train_df)\n",
    "\n",
    "train_df['preprocessed_headlines'] = preprocessed_headlines\n",
    "train_df['pos_tags_headlines'] = pos_tags_final_headlines\n",
    "train_df['er_tags_headlines'] = er_final_headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the articles\n",
    "pos_tags_final_articles, er_final_articles, preprocessed_articles = preprocessing(article, 'articles', train_df)\n",
    "\n",
    "train_df['preprocessed_articles'] = preprocessed_articles\n",
    "train_df['pos_tags_articles'] = pos_tags_final_articles\n",
    "train_df['er_tags_articles'] = er_final_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>headlines</th>\n",
       "      <th>articles</th>\n",
       "      <th>preprocessed_headlines</th>\n",
       "      <th>pos_tags_headlines</th>\n",
       "      <th>er_tags_headlines</th>\n",
       "      <th>preprocessed_articles</th>\n",
       "      <th>pos_tags_articles</th>\n",
       "      <th>er_tags_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3121</td>\n",
       "      <td>The Pope Says 'Humanity Must Repent For Abusin...</td>\n",
       "      <td>We humans “must repent and modify our lifestyl...</td>\n",
       "      <td>the pope say humanity must repent for abuse mo...</td>\n",
       "      <td>[(the, DT), (pope, NN), (say, VBP), (humanity,...</td>\n",
       "      <td>[]</td>\n",
       "      <td>we human must repent and modify our lifestyle ...</td>\n",
       "      <td>[(we, PRP), (human, NN), (must, MD), (repent, ...</td>\n",
       "      <td>[((thursday), DATE, 391), ((the, world, day), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3135</td>\n",
       "      <td>Russian allies are unknowingly working against...</td>\n",
       "      <td>The Secretary of State for Defence described P...</td>\n",
       "      <td>russian ally be unknowingly work against they ...</td>\n",
       "      <td>[(russian, JJ), (ally, NN), (be, VB), (unknowi...</td>\n",
       "      <td>[((russian), NORP, 381)]</td>\n",
       "      <td>the secretary of state for defence describe pu...</td>\n",
       "      <td>[(the, DT), (secretary, NNP), (of, IN), (state...</td>\n",
       "      <td>[((putin), PERSON, 380), ((russian), NORP, 381...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3134</td>\n",
       "      <td>I know this week has been disruptive, says LIZ...</td>\n",
       "      <td>The United Kingdom is the greatest country on ...</td>\n",
       "      <td>know this week have be disruptive say liz truss</td>\n",
       "      <td>[(know, VB), (this, DT), (week, NN), (have, VB...</td>\n",
       "      <td>[((this, week), DATE, 391)]</td>\n",
       "      <td>the united kingdom be the great country on ear...</td>\n",
       "      <td>[(the, DT), (united, NNP), (kingdom, NNP), (be...</td>\n",
       "      <td>[((the, united, kingdom), GPE, 384), ((70, yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3120</td>\n",
       "      <td>Vladimir Putin facing Ukraine 'humiliation' as...</td>\n",
       "      <td>VLADIMIR PUTIN is facing \"humiliation\" in Ukra...</td>\n",
       "      <td>vladimir putin face ukraine humiliation as rus...</td>\n",
       "      <td>[(vladimir, NNP), (putin, NNP), (face, NNP), (...</td>\n",
       "      <td>[((vladimir, putin), PERSON, 380), ((ukraine),...</td>\n",
       "      <td>vladimir putin be face humiliation in ukraine ...</td>\n",
       "      <td>[(vladimir, NNP), (putin, NNP), (be, VB), (fac...</td>\n",
       "      <td>[((vladimir, putin), PERSON, 380), ((ukraine),...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3136</td>\n",
       "      <td>What Are the Odds Putin is Bluffing About Usin...</td>\n",
       "      <td>Eurointelligence founder Wolfgang Münchau has ...</td>\n",
       "      <td>what be the odd putin be bluff about use nucle...</td>\n",
       "      <td>[(what, WP), (be, VB), (the, DT), (odd, JJ), (...</td>\n",
       "      <td>[]</td>\n",
       "      <td>eurointelligence founder wolfgang münchau have...</td>\n",
       "      <td>[(eurointelligence, NN), (founder, NN), (wolfg...</td>\n",
       "      <td>[((wolfgang, münchau), PERSON, 380), ((putin),...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id                                          headlines  \\\n",
       "0  3121  The Pope Says 'Humanity Must Repent For Abusin...   \n",
       "1  3135  Russian allies are unknowingly working against...   \n",
       "2  3134  I know this week has been disruptive, says LIZ...   \n",
       "3  3120  Vladimir Putin facing Ukraine 'humiliation' as...   \n",
       "4  3136  What Are the Odds Putin is Bluffing About Usin...   \n",
       "\n",
       "                                            articles  \\\n",
       "0  We humans “must repent and modify our lifestyl...   \n",
       "1  The Secretary of State for Defence described P...   \n",
       "2  The United Kingdom is the greatest country on ...   \n",
       "3  VLADIMIR PUTIN is facing \"humiliation\" in Ukra...   \n",
       "4  Eurointelligence founder Wolfgang Münchau has ...   \n",
       "\n",
       "                              preprocessed_headlines  \\\n",
       "0  the pope say humanity must repent for abuse mo...   \n",
       "1  russian ally be unknowingly work against they ...   \n",
       "2    know this week have be disruptive say liz truss   \n",
       "3  vladimir putin face ukraine humiliation as rus...   \n",
       "4  what be the odd putin be bluff about use nucle...   \n",
       "\n",
       "                                  pos_tags_headlines  \\\n",
       "0  [(the, DT), (pope, NN), (say, VBP), (humanity,...   \n",
       "1  [(russian, JJ), (ally, NN), (be, VB), (unknowi...   \n",
       "2  [(know, VB), (this, DT), (week, NN), (have, VB...   \n",
       "3  [(vladimir, NNP), (putin, NNP), (face, NNP), (...   \n",
       "4  [(what, WP), (be, VB), (the, DT), (odd, JJ), (...   \n",
       "\n",
       "                                   er_tags_headlines  \\\n",
       "0                                                 []   \n",
       "1                           [((russian), NORP, 381)]   \n",
       "2                        [((this, week), DATE, 391)]   \n",
       "3  [((vladimir, putin), PERSON, 380), ((ukraine),...   \n",
       "4                                                 []   \n",
       "\n",
       "                               preprocessed_articles  \\\n",
       "0  we human must repent and modify our lifestyle ...   \n",
       "1  the secretary of state for defence describe pu...   \n",
       "2  the united kingdom be the great country on ear...   \n",
       "3  vladimir putin be face humiliation in ukraine ...   \n",
       "4  eurointelligence founder wolfgang münchau have...   \n",
       "\n",
       "                                   pos_tags_articles  \\\n",
       "0  [(we, PRP), (human, NN), (must, MD), (repent, ...   \n",
       "1  [(the, DT), (secretary, NNP), (of, IN), (state...   \n",
       "2  [(the, DT), (united, NNP), (kingdom, NNP), (be...   \n",
       "3  [(vladimir, NNP), (putin, NNP), (be, VB), (fac...   \n",
       "4  [(eurointelligence, NN), (founder, NN), (wolfg...   \n",
       "\n",
       "                                    er_tags_articles  \n",
       "0  [((thursday), DATE, 391), ((the, world, day), ...  \n",
       "1  [((putin), PERSON, 380), ((russian), NORP, 381...  \n",
       "2  [((the, united, kingdom), GPE, 384), ((70, yea...  \n",
       "3  [((vladimir, putin), PERSON, 380), ((ukraine),...  \n",
       "4  [((wolfgang, münchau), PERSON, 380), ((putin),...  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting tweets_df into a csv file\n",
    "filename = '/Users/nitanshjain/Documents/Projects/Sem_Eval/semeval2023task3/preprocessed_data/prev_data/en_train_subtask_1.csv'\n",
    "train_df.to_csv(filename, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sem_Eval-qQJYuRaW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5bd67ae72067af90d04bfe162c4dbae485f2bc9cd35e8ebfff36ec4914b703f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
