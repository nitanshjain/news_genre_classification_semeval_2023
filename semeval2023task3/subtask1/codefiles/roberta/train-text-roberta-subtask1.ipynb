{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-27 21:36:21.903543: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/nitanshjain/.local/share/virtualenvs/Sem_Eval-qQJYuRaW/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import keras\n",
    "import torchtext\n",
    "\n",
    "import nlpaug\n",
    "import nlpaug.augmenter.char as nac\n",
    "import nlpaug.augmenter.word as naw\n",
    "import nlpaug.augmenter.sentence as nas\n",
    "import nlpaug.flow as nafc\n",
    "from nlpaug.util import Action\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "seed = 2000\n",
    "np.random.seed(seed)\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Flatten, Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "import torch\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification, AdamW\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(433, 10)\n",
      "opinion      382\n",
      "reporting     41\n",
      "satire        10\n",
      "Name: genre, dtype: int64\n",
      "(1437, 11)\n",
      "opinion      1073\n",
      "reporting     271\n",
      "satire         93\n",
      "Name: genre, dtype: int64\n",
      "(83, 10)\n",
      "reporting    54\n",
      "opinion      20\n",
      "satire        9\n",
      "Name: genre, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_sub1_df = pd.read_csv('/Users/nitanshjain/Documents/Projects/Sem_Eval/semeval2023task3/preprocessed_data/subtask1/en_train_subtask_1.csv')\n",
    "print(train_sub1_df.shape)\n",
    "train_sub1_df.head()\n",
    "print(train_sub1_df.genre.value_counts())\n",
    "\n",
    "final_train_sub1_df = pd.read_csv('/Users/nitanshjain/Documents/Projects/Sem_Eval/semeval2023task3/preprocessed_data/subtask1/final_train_subtask_1.csv')\n",
    "print(final_train_sub1_df.shape)\n",
    "final_train_sub1_df.head()\n",
    "print(final_train_sub1_df.genre.value_counts())\n",
    "\n",
    "dev_sub1_df = pd.read_csv('/Users/nitanshjain/Documents/Projects/Sem_Eval/semeval2023task3/preprocessed_data/subtask1/en_dev_subtask_1.csv')\n",
    "print(dev_sub1_df.shape)\n",
    "dev_sub1_df.head()\n",
    "print(dev_sub1_df.genre.value_counts())\n",
    "\n",
    "# test_sub1_df = pd.read_csv('/Users/nitanshjain/Documents/Projects/Sem_Eval/semeval2023task3/preprocessed_data/extra_test.csv')\n",
    "# print(test_sub1_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    382\n",
      "1     41\n",
      "2     10\n",
      "Name: genre, dtype: int64\n",
      "0    1073\n",
      "1     271\n",
      "2      93\n",
      "Name: genre, dtype: int64\n",
      "1    54\n",
      "0    20\n",
      "2     9\n",
      "Name: genre, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>genre</th>\n",
       "      <th>headlines</th>\n",
       "      <th>articles</th>\n",
       "      <th>preprocessed_headlines</th>\n",
       "      <th>pos_tags_headlines</th>\n",
       "      <th>er_tags_headlines</th>\n",
       "      <th>preprocessed_articles</th>\n",
       "      <th>pos_tags_articles</th>\n",
       "      <th>er_tags_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>820791520</td>\n",
       "      <td>1</td>\n",
       "      <td>George III Lost America.\\n</td>\n",
       "      <td>Theresa May Could Lose the United Kingdom Over...</td>\n",
       "      <td>george iii lose america</td>\n",
       "      <td>[(george, 'NNP'), (iii, 'NNP'), (lose, 'VB'), ...</td>\n",
       "      <td>[(george iii, 'PERSON', 380), (america, 'GPE',...</td>\n",
       "      <td>theresa may could lose the united kingdom over...</td>\n",
       "      <td>[(theresa, 'NN'), (may, 'MD'), (could, 'MD'), ...</td>\n",
       "      <td>[(the united kingdom, 'GPE', 384), (the europe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>821040551</td>\n",
       "      <td>1</td>\n",
       "      <td>Queen Elizabeth Would Be Evacuated in Event of...</td>\n",
       "      <td>If Britain leaves the European Union without a...</td>\n",
       "      <td>queen elizabeth would be evacuate in event of ...</td>\n",
       "      <td>[(queen, 'NNP'), (elizabeth, 'NNP'), (would, '...</td>\n",
       "      <td>[(elizabeth, 'PERSON', 380), (brexit riot repo...</td>\n",
       "      <td>if britain leave the european union without tr...</td>\n",
       "      <td>[(if, 'IN'), (britain, 'NNP'), (leave, 'VBP'),...</td>\n",
       "      <td>[(britain, 'GPE', 384), (the european union, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>813552066</td>\n",
       "      <td>1</td>\n",
       "      <td>You insult us, ambassador: Woody Johnson flagr...</td>\n",
       "      <td>With three months until Britain leaves the Eur...</td>\n",
       "      <td>you insult us ambassador woody johnson flagran...</td>\n",
       "      <td>[(you, 'PRP'), (insult, 'VBP'), (us, 'NNP'), (...</td>\n",
       "      <td>[(woody johnson, 'PERSON', 380), (peter, 'PERS...</td>\n",
       "      <td>with three month until britain leave the europ...</td>\n",
       "      <td>[(with, 'IN'), (three, 'CD'), (month, 'NN'), (...</td>\n",
       "      <td>[(three month, 'DATE', 391), (britain, 'GPE', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>817176202</td>\n",
       "      <td>1</td>\n",
       "      <td>The British People, as Well as the Politicians...</td>\n",
       "      <td>The British Parliament just handed Prime Minis...</td>\n",
       "      <td>the british people as well as the politician d...</td>\n",
       "      <td>[(the, 'DT'), (british, 'JJ'), (people, 'NNS')...</td>\n",
       "      <td>[(british, 'NORP', 381)]</td>\n",
       "      <td>the british parliament just hand prime ministe...</td>\n",
       "      <td>[(the, 'DT'), (british, 'JJ'), (parliament, 'N...</td>\n",
       "      <td>[(british, 'NORP', 381), (british, 'NORP', 381...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>820419869</td>\n",
       "      <td>1</td>\n",
       "      <td>No break from Brexit: RT takes a look at lates...</td>\n",
       "      <td>As British MPs are told that their February br...</td>\n",
       "      <td>no break from brexit rt take look at late deve...</td>\n",
       "      <td>[(no, 'DT'), (break, 'NN'), (from, 'IN'), (bre...</td>\n",
       "      <td>[(brexit rt, 'ORG', 383)]</td>\n",
       "      <td>as british mp be tell that their february brea...</td>\n",
       "      <td>[(as, 'IN'), (british, 'NNP'), (mp, 'NNP'), (b...</td>\n",
       "      <td>[(british, 'NORP', 381), (february, 'DATE', 39...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  genre                                          headlines  \\\n",
       "0  820791520      1                         George III Lost America.\\n   \n",
       "1  821040551      1  Queen Elizabeth Would Be Evacuated in Event of...   \n",
       "2  813552066      1  You insult us, ambassador: Woody Johnson flagr...   \n",
       "3  817176202      1  The British People, as Well as the Politicians...   \n",
       "4  820419869      1  No break from Brexit: RT takes a look at lates...   \n",
       "\n",
       "                                            articles  \\\n",
       "0  Theresa May Could Lose the United Kingdom Over...   \n",
       "1  If Britain leaves the European Union without a...   \n",
       "2  With three months until Britain leaves the Eur...   \n",
       "3  The British Parliament just handed Prime Minis...   \n",
       "4  As British MPs are told that their February br...   \n",
       "\n",
       "                              preprocessed_headlines  \\\n",
       "0                            george iii lose america   \n",
       "1  queen elizabeth would be evacuate in event of ...   \n",
       "2  you insult us ambassador woody johnson flagran...   \n",
       "3  the british people as well as the politician d...   \n",
       "4  no break from brexit rt take look at late deve...   \n",
       "\n",
       "                                  pos_tags_headlines  \\\n",
       "0  [(george, 'NNP'), (iii, 'NNP'), (lose, 'VB'), ...   \n",
       "1  [(queen, 'NNP'), (elizabeth, 'NNP'), (would, '...   \n",
       "2  [(you, 'PRP'), (insult, 'VBP'), (us, 'NNP'), (...   \n",
       "3  [(the, 'DT'), (british, 'JJ'), (people, 'NNS')...   \n",
       "4  [(no, 'DT'), (break, 'NN'), (from, 'IN'), (bre...   \n",
       "\n",
       "                                   er_tags_headlines  \\\n",
       "0  [(george iii, 'PERSON', 380), (america, 'GPE',...   \n",
       "1  [(elizabeth, 'PERSON', 380), (brexit riot repo...   \n",
       "2  [(woody johnson, 'PERSON', 380), (peter, 'PERS...   \n",
       "3                           [(british, 'NORP', 381)]   \n",
       "4                          [(brexit rt, 'ORG', 383)]   \n",
       "\n",
       "                               preprocessed_articles  \\\n",
       "0  theresa may could lose the united kingdom over...   \n",
       "1  if britain leave the european union without tr...   \n",
       "2  with three month until britain leave the europ...   \n",
       "3  the british parliament just hand prime ministe...   \n",
       "4  as british mp be tell that their february brea...   \n",
       "\n",
       "                                   pos_tags_articles  \\\n",
       "0  [(theresa, 'NN'), (may, 'MD'), (could, 'MD'), ...   \n",
       "1  [(if, 'IN'), (britain, 'NNP'), (leave, 'VBP'),...   \n",
       "2  [(with, 'IN'), (three, 'CD'), (month, 'NN'), (...   \n",
       "3  [(the, 'DT'), (british, 'JJ'), (parliament, 'N...   \n",
       "4  [(as, 'IN'), (british, 'NNP'), (mp, 'NNP'), (b...   \n",
       "\n",
       "                                    er_tags_articles  \n",
       "0  [(the united kingdom, 'GPE', 384), (the europe...  \n",
       "1  [(britain, 'GPE', 384), (the european union, '...  \n",
       "2  [(three month, 'DATE', 391), (britain, 'GPE', ...  \n",
       "3  [(british, 'NORP', 381), (british, 'NORP', 381...  \n",
       "4  [(british, 'NORP', 381), (february, 'DATE', 39...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "train_sub1_df['genre'] = le.fit_transform(train_sub1_df['genre'])\n",
    "print(train_sub1_df.genre.value_counts())\n",
    "\n",
    "le = LabelEncoder()\n",
    "final_train_sub1_df['genre'] = le.fit_transform(final_train_sub1_df['genre'])\n",
    "print(final_train_sub1_df.genre.value_counts())\n",
    "\n",
    "le = LabelEncoder()\n",
    "dev_sub1_df['genre'] = le.fit_transform(dev_sub1_df['genre'])\n",
    "print(dev_sub1_df.genre.value_counts())\n",
    "dev_sub1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    159\n",
      "1    153\n",
      "2     93\n",
      "Name: genre, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(405, 11)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n0 = 914\n",
    "n1 = 118\n",
    "final_train_sub1_df = final_train_sub1_df.drop(final_train_sub1_df[final_train_sub1_df['genre'].eq(0)].sample(n0).index)\n",
    "final_train_sub1_df = final_train_sub1_df.drop(final_train_sub1_df[final_train_sub1_df['genre'].eq(1)].sample(n1).index)\n",
    "print(final_train_sub1_df.genre.value_counts())\n",
    "final_train_sub1_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    159\n",
       "1    153\n",
       "2     93\n",
       "Name: genre, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_df = pd.DataFrame(final_train_sub1_df.loc[final_train_sub1_df['genre']==0])\n",
    "new_train_df = new_train_df.append(final_train_sub1_df.loc[final_train_sub1_df['genre']==1])\n",
    "new_train_df = new_train_df.append(final_train_sub1_df.loc[final_train_sub1_df['genre']==2])\n",
    "new_train_df.genre.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.8490566037735849, 1: 0.8823529411764706, 2: 1.4516129032258065}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight = \"balanced\",\n",
    "    classes = np.unique(new_train_df.genre),\n",
    "    y = new_train_df.genre\n",
    ")\n",
    "\n",
    "class_weights = dict(zip(np.unique(new_train_df.genre), class_weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug = naw.SynonymAug(aug_src='wordnet',aug_max=3)\n",
    "aug = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action=\"substitute\", aug_max=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = new_train_df.preprocessed_articles + new_train_df.preprocessed_headlines\n",
    "y_train = new_train_df[['genre']]\n",
    "\n",
    "x_dev = dev_sub1_df.preprocessed_articles + dev_sub1_df.preprocessed_headlines\n",
    "y_dev = dev_sub1_df[['genre']]\n",
    "\n",
    "# x_train = new_train_df.text\n",
    "# y_train = new_train_df.target\n",
    "\n",
    "# x_test = new_train_df.text\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base', num_labels=3, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running count =  10\n",
      "Running count =  20\n",
      "Running count =  30\n",
      "Running count =  40\n",
      "Running count =  50\n",
      "Running count =  60\n",
      "Running count =  70\n",
      "Running count =  80\n",
      "Running count =  90\n",
      "Running count =  100\n",
      "Running count =  110\n",
      "Running count =  120\n",
      "Running count =  130\n",
      "Running count =  140\n",
      "Running count =  150\n",
      "Running count =  160\n",
      "Running count =  170\n",
      "Running count =  180\n",
      "Running count =  190\n",
      "Running count =  200\n",
      "Running count =  210\n",
      "Running count =  220\n",
      "Running count =  230\n",
      "Running count =  240\n",
      "Running count =  250\n",
      "Running count =  260\n",
      "Running count =  270\n",
      "Running count =  280\n",
      "Running count =  290\n",
      "Running count =  300\n",
      "Running count =  310\n",
      "Running count =  320\n",
      "Running count =  330\n",
      "Running count =  340\n",
      "Running count =  350\n",
      "Running count =  360\n",
      "Running count =  370\n",
      "Running count =  380\n",
      "Running count =  390\n",
      "Running count =  400\n"
     ]
    }
   ],
   "source": [
    "augmented_sentences=[]\n",
    "augmented_sentences_labels=[]\n",
    "count = 0\n",
    "for i in x_train.index:\n",
    "    count+=1\n",
    "    if count%10==0:\n",
    "        print(\"Running count = \", count)\n",
    "    if y_train.genre[i]==0:\n",
    "        try:\n",
    "            temps=aug.augment(x_train[i], n=2)\n",
    "            for sent in temps:\n",
    "                augmented_sentences.append(sent)\n",
    "                augmented_sentences_labels.append(0)\n",
    "        except:\n",
    "            continue\n",
    "    if y_train.genre[i]==1:\n",
    "        try:\n",
    "            temps=aug.augment(x_train[i], n=2)\n",
    "            for sent in temps:\n",
    "                augmented_sentences.append(sent)\n",
    "                augmented_sentences_labels.append(1)\n",
    "        except:\n",
    "            continue\n",
    "    if y_train.genre[i]==2:\n",
    "        try:\n",
    "            temps=aug.augment(x_train[i], n=2)\n",
    "            for sent in temps:\n",
    "                augmented_sentences.append(sent)\n",
    "                augmented_sentences_labels.append(2)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_aug = pd.Series()\n",
    "y_train_aug = pd.Series()\n",
    "x_train_aug = x_train_aug.append(pd.Series(augmented_sentences), ignore_index=True)\n",
    "y_train_aug = y_train_aug.append(pd.Series(augmented_sentences_labels), ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train_aug.to_numpy().reshape(-1)\n",
    "y_train = y_train_aug.to_numpy().reshape(-1,1)\n",
    "x_dev = x_dev.to_numpy().reshape(-1)\n",
    "y_dev = y_dev.to_numpy().reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roberta_encode(texts, tokenizer):\n",
    "    MAX_LEN = 512\n",
    "    ct = len(texts)\n",
    "    input_ids = np.ones((ct, MAX_LEN), dtype='int32')\n",
    "    attention_mask = np.zeros((ct, MAX_LEN), dtype='int32')\n",
    "    token_type_ids = np.zeros((ct, MAX_LEN), dtype='int32') # Not used in text classification\n",
    "\n",
    "    for k, text in enumerate(texts):\n",
    "        # Tokenize\n",
    "        tok_text = tokenizer.tokenize(str(text))\n",
    "        \n",
    "        # Truncate and convert tokens to numerical IDs\n",
    "        enc_text = tokenizer.convert_tokens_to_ids(tok_text[:(MAX_LEN-2)])\n",
    "        \n",
    "        input_length = len(enc_text) + 2\n",
    "        input_length = input_length if input_length < MAX_LEN else MAX_LEN\n",
    "        \n",
    "        # Add tokens [CLS] and [SEP] at the beginning and the end\n",
    "        input_ids[k,:input_length] = np.asarray([0] + enc_text + [2], dtype='int32')\n",
    "        \n",
    "        # Set to 1s in the attention input\n",
    "        attention_mask[k,:input_length] = 1\n",
    "\n",
    "    return {\n",
    "        'input_word_ids': input_ids,\n",
    "        'input_mask': attention_mask,\n",
    "        'input_type_ids': token_type_ids\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = roberta_encode(x_train, tokenizer)\n",
    "x_dev = roberta_encode(x_dev, tokenizer)\n",
    "\n",
    "y_train = np.asarray(y_train, dtype='int32')\n",
    "y_dev = np.asarray(y_dev, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_categories):\n",
    "    MAX_LEN = 512\n",
    "    input_word_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_word_ids')\n",
    "    input_mask = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_mask')\n",
    "    input_type_ids = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name='input_type_ids')\n",
    "    \n",
    "    \n",
    "\n",
    "    # Import RoBERTa model from HuggingFace\n",
    "    roberta_model = TFRobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "    # custom_objects = {\"TFRobertaForSequenceClassification\": TFRobertaForSequenceClassification}\n",
    "    # config = roberta_model_copy.get_config()\n",
    "    # with tf.keras.utils.custom_object_scope(custom_objects):\n",
    "    #     roberta_model = TFRobertaForSequenceClassification.from_config(config)\n",
    "    x = roberta_model(input_word_ids, attention_mask=input_mask, token_type_ids=input_type_ids)\n",
    "\n",
    "    # Huggingface transformers have multiple outputs, em\n",
    "    # beddings are the first one,\n",
    "    # so let's slice out the first position\n",
    "    x = x[0]\n",
    "\n",
    "    x = tf.keras.layers.Dropout(rate=0.1)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(n_categories, activation='softmax')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=x)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.experimental.Adam(learning_rate=0.00001),\n",
    "        loss=SparseCategoricalFocalLoss(gamma=2),\n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-27 22:09:49.960942: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_word_ids (InputLayer)    [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_mask (InputLayer)        [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_type_ids (InputLayer)    [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_for_sequence_classi  TFSequenceClassifie  124647170  ['input_word_ids[0][0]',         \n",
      " fication (TFRobertaForSequence  rOutput(loss=None,               'input_mask[0][0]',             \n",
      " Classification)                logits=(None, 2),                 'input_type_ids[0][0]']         \n",
      "                                 hidden_states=None                                               \n",
      "                                , attentions=None)                                                \n",
      "                                                                                                  \n",
      " dropout_38 (Dropout)           (None, 2)            0           ['tf_roberta_for_sequence_classif\n",
      "                                                                 ication[0][0]']                  \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2)            0           ['dropout_38[0][0]']             \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 256)          768         ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 3)            771         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 124,648,709\n",
      "Trainable params: 124,648,709\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_copy = build_model(3)\n",
    "model_copy.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "51/51 [==============================] - 7088s 139s/step - loss: 0.2742 - accuracy: 0.7512 - val_loss: 0.5242 - val_accuracy: 0.3735\n",
      "Epoch 2/5\n",
      "51/51 [==============================] - 7055s 138s/step - loss: 0.2900 - accuracy: 0.7201 - val_loss: 0.4040 - val_accuracy: 0.5060\n",
      "Epoch 3/5\n",
      "51/51 [==============================] - 7031s 138s/step - loss: 0.1759 - accuracy: 0.8308 - val_loss: 0.4270 - val_accuracy: 0.5783\n",
      "Epoch 4/5\n",
      "51/51 [==============================] - 7039s 138s/step - loss: 0.1358 - accuracy: 0.8657 - val_loss: 0.4811 - val_accuracy: 0.5663\n",
      "Epoch 5/5\n",
      "51/51 [==============================] - 7005s 137s/step - loss: 0.0969 - accuracy: 0.9129 - val_loss: 0.5203 - val_accuracy: 0.5663\n",
      "Epoch 1/5\n",
      "51/51 [==============================] - 6980s 137s/step - loss: 0.0688 - accuracy: 0.9303 - val_loss: 0.7421 - val_accuracy: 0.4096\n",
      "Epoch 2/5\n",
      "15/51 [=======>......................] - ETA: 1:32:27 - loss: 0.0856 - accuracy: 0.9083"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "checkpoint_filepath = '/Users/nitanshjain/Documents/Projects/Sem_Eval/semeval2023task3/codefiles/subtask1/roberta/weights/weights-improvement-multimodal-h5-{epoch:02d}-{val_accuracy:.2f}.h5'\n",
    "\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='val_accuracy', \n",
    "    save_best_only=False,\n",
    "    save_weights_only=False,\n",
    "    mode='max'\n",
    "    )\n",
    "\n",
    "reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.2,\n",
    "                              patience=10, min_lr=0.0001)\n",
    "\n",
    "model_copy.fit(x=x_train,\n",
    "                y=y_train,\n",
    "                batch_size=batch_size,\n",
    "                epochs=10,\n",
    "                callbacks=[model_checkpoint_callback, reduce_lr_callback],\n",
    "                validation_data=(x_dev, y_dev),\n",
    "                shuffle=True,\n",
    "                verbose=1, class_weight=class_weights\n",
    "                )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "with tf.keras.utils.CustomObjectScope({'TFRobertaForSequenceClassification': TFRobertaForSequenceClassification.from_pretrained('roberta-base')}):\n",
    "    model = load_model('/content/weights-improvement-multimodal-h5-01-0.57.h5')  \n",
    "model.summary(print_fn=print)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 84s 24s/step\n",
      "[[0.46505716 0.27893054 0.25601232]\n",
      " [0.26064035 0.5052562  0.23410344]\n",
      " [0.4774259  0.3284097  0.1941644 ]\n",
      " [0.43730757 0.3376547  0.22503771]\n",
      " [0.25886112 0.5445365  0.1966024 ]\n",
      " [0.47891247 0.28328687 0.23780061]\n",
      " [0.31777605 0.47541195 0.20681195]\n",
      " [0.35552657 0.3245846  0.3198889 ]\n",
      " [0.321076   0.38235837 0.29656565]\n",
      " [0.25898972 0.544331   0.19667925]\n",
      " [0.30466628 0.51337916 0.18195464]\n",
      " [0.31616122 0.48571527 0.19812343]\n",
      " [0.43953463 0.270253   0.2902123 ]\n",
      " [0.2839397  0.47513548 0.24092482]\n",
      " [0.36571234 0.45796764 0.17632   ]\n",
      " [0.36682653 0.45377678 0.17939678]\n",
      " [0.2602805  0.5375579  0.2021616 ]\n",
      " [0.26298368 0.46841362 0.2686027 ]\n",
      " [0.2958601  0.5178372  0.18630266]\n",
      " [0.3151076  0.5044825  0.18040988]\n",
      " [0.29034206 0.50789446 0.20176345]\n",
      " [0.33625668 0.48749006 0.17625321]\n",
      " [0.33169404 0.4872298  0.1810761 ]\n",
      " [0.42655975 0.36401898 0.20942126]\n",
      " [0.2863524  0.5267157  0.18693194]\n",
      " [0.27097887 0.53918374 0.18983741]\n",
      " [0.26433808 0.54529387 0.19036806]\n",
      " [0.35279977 0.4715274  0.1756728 ]\n",
      " [0.27364898 0.5337316  0.19261952]\n",
      " [0.2606787  0.4707309  0.26859042]\n",
      " [0.4154357  0.28300947 0.30155483]\n",
      " [0.29524183 0.5145799  0.19017826]\n",
      " [0.28504547 0.5207612  0.19419341]\n",
      " [0.3179523  0.4995427  0.18250495]\n",
      " [0.4257336  0.39046535 0.18380105]\n",
      " [0.28547552 0.5252439  0.18928057]\n",
      " [0.30883753 0.50723714 0.18392529]\n",
      " [0.29340076 0.52209157 0.18450758]\n",
      " [0.3559063  0.46466494 0.17942876]\n",
      " [0.30975178 0.5062043  0.18404388]\n",
      " [0.30473852 0.5130549  0.1822066 ]\n",
      " [0.41815627 0.25155497 0.33028877]\n",
      " [0.30703264 0.5118498  0.1811176 ]\n",
      " [0.3342097  0.48655066 0.17923959]\n",
      " [0.3329787  0.42543077 0.2415905 ]\n",
      " [0.43504825 0.37591192 0.1890399 ]\n",
      " [0.3372204  0.23903765 0.42374197]\n",
      " [0.44080657 0.26481384 0.2943796 ]\n",
      " [0.3758046  0.44690248 0.17729294]\n",
      " [0.4384954  0.38055    0.18095464]\n",
      " [0.33613104 0.48341128 0.18045773]\n",
      " [0.44116524 0.2683002  0.29053456]\n",
      " [0.3230331  0.49753916 0.17942764]\n",
      " [0.3557214  0.4648418  0.1794368 ]\n",
      " [0.434531   0.26245984 0.3030091 ]\n",
      " [0.45425928 0.33955303 0.20618767]\n",
      " [0.3171585  0.31221327 0.37062824]\n",
      " [0.3300634  0.43430072 0.2356359 ]\n",
      " [0.48263526 0.32477927 0.19258548]\n",
      " [0.2624255  0.5043481  0.23322637]\n",
      " [0.42663014 0.30470318 0.26866668]\n",
      " [0.32493943 0.49824807 0.17681244]\n",
      " [0.32955602 0.4946232  0.17582068]\n",
      " [0.3175982  0.50139004 0.1810118 ]\n",
      " [0.4572202  0.27344117 0.26933873]\n",
      " [0.39663818 0.39753628 0.20582551]\n",
      " [0.2831279  0.52533996 0.1915321 ]\n",
      " [0.26203844 0.46819448 0.26976708]\n",
      " [0.32685566 0.3411046  0.33203968]\n",
      " [0.29172835 0.520356   0.1879157 ]\n",
      " [0.50185966 0.304698   0.19344227]\n",
      " [0.28547865 0.52729994 0.18722144]\n",
      " [0.29225445 0.51637495 0.19137064]\n",
      " [0.29825705 0.51199496 0.189748  ]\n",
      " [0.38946298 0.4086613  0.2018757 ]\n",
      " [0.41420206 0.33376056 0.2520374 ]\n",
      " [0.25896034 0.5368487  0.20419091]\n",
      " [0.27511472 0.5248807  0.20000458]\n",
      " [0.43857113 0.2749254  0.28650343]\n",
      " [0.4342434  0.26364473 0.30211192]\n",
      " [0.30026135 0.50473666 0.19500194]\n",
      " [0.33798116 0.48359632 0.17842245]\n",
      " [0.42985877 0.30657333 0.26356795]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_copy.predict(x_dev)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 2, 0, 1, 0, 1, 0, 1, 1, 0, 0, 2, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0]\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [2]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [0]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [2]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_final = list()\n",
    "y_pred_sub = list()\n",
    "for val in y_pred:\n",
    "    if val[0]>val[1] and val[0]>val[2]:\n",
    "        y_pred_final.append(0)\n",
    "        y_pred_sub.append('opinion')\n",
    "    elif val[1]>val[0] and val[1]>val[2]:\n",
    "        y_pred_final.append(1)\n",
    "        y_pred_sub.append('reporting')\n",
    "    else:\n",
    "        y_pred_final.append(2)\n",
    "        y_pred_sub.append('satire')\n",
    "\n",
    "print(y_pred_final) \n",
    "print(y_dev) \n",
    "# print(list(y_dev))  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro f1_score = 0.3794703794703795\n",
      "Micro f1_score = 0.6144578313253012\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print('Macro f1_score = {}'.format(f1_score(y_dev, y_pred_final, average='macro')))\n",
    "print('Micro f1_score = {}'.format(f1_score(y_dev, y_pred_final, average='micro')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dev_id = dev_sub1_df.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>820791520</th>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>821040551</th>\n",
       "      <td>reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>813552066</th>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>817176202</th>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820419869</th>\n",
       "      <td>reporting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                class\n",
       "article_id           \n",
       "820791520     opinion\n",
       "821040551   reporting\n",
       "813552066     opinion\n",
       "817176202     opinion\n",
       "820419869   reporting"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict = {'article_id': y_dev_id, 'class': y_pred_sub} \n",
    "submit_df = pd.DataFrame(dict) \n",
    "submit_df.set_index('article_id', inplace=True)\n",
    "submit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_df.to_csv('2_data_25_16.txt', sep='\\t')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>headlines</th>\n",
       "      <th>articles</th>\n",
       "      <th>preprocessed_headlines</th>\n",
       "      <th>pos_tags_headlines</th>\n",
       "      <th>er_tags_headlines</th>\n",
       "      <th>preprocessed_articles</th>\n",
       "      <th>pos_tags_articles</th>\n",
       "      <th>er_tags_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3121</td>\n",
       "      <td>The Pope Says 'Humanity Must Repent For Abusin...</td>\n",
       "      <td>We humans “must repent and modify our lifestyl...</td>\n",
       "      <td>the pope say humanity must repent for abuse mo...</td>\n",
       "      <td>[(the, 'DT'), (pope, 'NN'), (say, 'VBP'), (hum...</td>\n",
       "      <td>[]</td>\n",
       "      <td>we human must repent and modify our lifestyle ...</td>\n",
       "      <td>[(we, 'PRP'), (human, 'NN'), (must, 'MD'), (re...</td>\n",
       "      <td>[(thursday, 'DATE', 391), (the world day, 'DAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3135</td>\n",
       "      <td>Russian allies are unknowingly working against...</td>\n",
       "      <td>The Secretary of State for Defence described P...</td>\n",
       "      <td>russian ally be unknowingly work against they ...</td>\n",
       "      <td>[(russian, 'JJ'), (ally, 'NN'), (be, 'VB'), (u...</td>\n",
       "      <td>[(russian, 'NORP', 381)]</td>\n",
       "      <td>the secretary of state for defence describe pu...</td>\n",
       "      <td>[(the, 'DT'), (secretary, 'NNP'), (of, 'IN'), ...</td>\n",
       "      <td>[(putin, 'PERSON', 380), (russian, 'NORP', 381...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3134</td>\n",
       "      <td>I know this week has been disruptive, says LIZ...</td>\n",
       "      <td>The United Kingdom is the greatest country on ...</td>\n",
       "      <td>know this week have be disruptive say liz truss</td>\n",
       "      <td>[(know, 'VB'), (this, 'DT'), (week, 'NN'), (ha...</td>\n",
       "      <td>[(this week, 'DATE', 391)]</td>\n",
       "      <td>the united kingdom be the great country on ear...</td>\n",
       "      <td>[(the, 'DT'), (united, 'NNP'), (kingdom, 'NNP'...</td>\n",
       "      <td>[(the united kingdom, 'GPE', 384), (year, 'DAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3120</td>\n",
       "      <td>Vladimir Putin facing Ukraine 'humiliation' as...</td>\n",
       "      <td>VLADIMIR PUTIN is facing \"humiliation\" in Ukra...</td>\n",
       "      <td>vladimir putin face ukraine humiliation as rus...</td>\n",
       "      <td>[(vladimir, 'NNP'), (putin, 'NNP'), (face, 'NN...</td>\n",
       "      <td>[(vladimir putin, 'PERSON', 380), (ukraine, 'G...</td>\n",
       "      <td>vladimir putin be face humiliation in ukraine ...</td>\n",
       "      <td>[(vladimir, 'NNP'), (putin, 'NNP'), (be, 'VB')...</td>\n",
       "      <td>[(vladimir putin, 'PERSON', 380), (ukraine, 'G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>3136</td>\n",
       "      <td>What Are the Odds Putin is Bluffing About Usin...</td>\n",
       "      <td>Eurointelligence founder Wolfgang Münchau has ...</td>\n",
       "      <td>what be the odd putin be bluff about use nucle...</td>\n",
       "      <td>[(what, 'WP'), (be, 'VB'), (the, 'DT'), (odd, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>eurointelligence founder wolfgang münchau have...</td>\n",
       "      <td>[(eurointelligence, 'NN'), (founder, 'NN'), (w...</td>\n",
       "      <td>[(wolfgang münchau, 'PERSON', 380), (putin, 'P...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0    id                                          headlines  \\\n",
       "0           0  3121  The Pope Says 'Humanity Must Repent For Abusin...   \n",
       "1           1  3135  Russian allies are unknowingly working against...   \n",
       "2           2  3134  I know this week has been disruptive, says LIZ...   \n",
       "3           3  3120  Vladimir Putin facing Ukraine 'humiliation' as...   \n",
       "4           4  3136  What Are the Odds Putin is Bluffing About Usin...   \n",
       "\n",
       "                                            articles  \\\n",
       "0  We humans “must repent and modify our lifestyl...   \n",
       "1  The Secretary of State for Defence described P...   \n",
       "2  The United Kingdom is the greatest country on ...   \n",
       "3  VLADIMIR PUTIN is facing \"humiliation\" in Ukra...   \n",
       "4  Eurointelligence founder Wolfgang Münchau has ...   \n",
       "\n",
       "                              preprocessed_headlines  \\\n",
       "0  the pope say humanity must repent for abuse mo...   \n",
       "1  russian ally be unknowingly work against they ...   \n",
       "2    know this week have be disruptive say liz truss   \n",
       "3  vladimir putin face ukraine humiliation as rus...   \n",
       "4  what be the odd putin be bluff about use nucle...   \n",
       "\n",
       "                                  pos_tags_headlines  \\\n",
       "0  [(the, 'DT'), (pope, 'NN'), (say, 'VBP'), (hum...   \n",
       "1  [(russian, 'JJ'), (ally, 'NN'), (be, 'VB'), (u...   \n",
       "2  [(know, 'VB'), (this, 'DT'), (week, 'NN'), (ha...   \n",
       "3  [(vladimir, 'NNP'), (putin, 'NNP'), (face, 'NN...   \n",
       "4  [(what, 'WP'), (be, 'VB'), (the, 'DT'), (odd, ...   \n",
       "\n",
       "                                   er_tags_headlines  \\\n",
       "0                                                 []   \n",
       "1                           [(russian, 'NORP', 381)]   \n",
       "2                         [(this week, 'DATE', 391)]   \n",
       "3  [(vladimir putin, 'PERSON', 380), (ukraine, 'G...   \n",
       "4                                                 []   \n",
       "\n",
       "                               preprocessed_articles  \\\n",
       "0  we human must repent and modify our lifestyle ...   \n",
       "1  the secretary of state for defence describe pu...   \n",
       "2  the united kingdom be the great country on ear...   \n",
       "3  vladimir putin be face humiliation in ukraine ...   \n",
       "4  eurointelligence founder wolfgang münchau have...   \n",
       "\n",
       "                                   pos_tags_articles  \\\n",
       "0  [(we, 'PRP'), (human, 'NN'), (must, 'MD'), (re...   \n",
       "1  [(the, 'DT'), (secretary, 'NNP'), (of, 'IN'), ...   \n",
       "2  [(the, 'DT'), (united, 'NNP'), (kingdom, 'NNP'...   \n",
       "3  [(vladimir, 'NNP'), (putin, 'NNP'), (be, 'VB')...   \n",
       "4  [(eurointelligence, 'NN'), (founder, 'NN'), (w...   \n",
       "\n",
       "                                    er_tags_articles  \n",
       "0  [(thursday, 'DATE', 391), (the world day, 'DAT...  \n",
       "1  [(putin, 'PERSON', 380), (russian, 'NORP', 381...  \n",
       "2  [(the united kingdom, 'GPE', 384), (year, 'DAT...  \n",
       "3  [(vladimir putin, 'PERSON', 380), (ukraine, 'G...  \n",
       "4  [(wolfgang münchau, 'PERSON', 380), (putin, 'P...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('/Users/nitanshjain/Documents/Projects/Sem_Eval/semeval2023task3/preprocessed_data/subtask1/en_test_subtask_1.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54, 10)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     the pope say humanity must repent for abuse mo...\n",
       "1     russian ally be unknowingly work against they ...\n",
       "2     know this week have be disruptive say liz trus...\n",
       "3     vladimir putin face ukraine humiliation as rus...\n",
       "4     what be the odd putin be bluff about use nucle...\n",
       "5     florida surgeon general issue warn for mrna co...\n",
       "6     biden 's secret promise to opec backfire shell...\n",
       "7     central bank add gold for fifth straight month...\n",
       "8     we deny preposterous claim they sabotage nord ...\n",
       "9     do uncle sam wile coyote blow up the wrong pip...\n",
       "10    european health official admit microsoft found...\n",
       "11    putin ally admit kyiv could soon bomb moscow a...\n",
       "12    fda withhold autopsy result on people who die ...\n",
       "13    russian plan reveal war with nato be comethis ...\n",
       "14    al gore compare climate denier to police in uv...\n",
       "15    dim witte duke prince harry slam for wade into...\n",
       "16    american pravda of pipeline and plagueeurope b...\n",
       "17    top un official we own the science and the wor...\n",
       "18    activist sign petition as at least  dead from ...\n",
       "19    nearly of people live in san francisco and sea...\n",
       "20    china mark milestone in new gen heavy duty roc...\n",
       "21    report  of border crosser bus to nyc remain in...\n",
       "22    oh thank god banksy be in ukrainethe war in uk...\n",
       "23     year old man sue his mother after be evict fr...\n",
       "24    energy crisis drive up cremation cost in franc...\n",
       "25    meta add to russia list of extremist and terro...\n",
       "26    opinion america bring back fascism so what now...\n",
       "27    solar panel drain the sun energy expert saythi...\n",
       "28    boris johnson to be remove from historyformer ...\n",
       "29    the gop war against lgbtqlet it be paul mccart...\n",
       "30    silent no more the story that must be tell abo...\n",
       "31    bolsonaro will not condemn putin say brazil wi...\n",
       "32    we set free over , illegal migrant with track ...\n",
       "33    how many and who be die for our freedomwhether...\n",
       "34    leader happy to pose with zelensky hesitant on...\n",
       "35    student lose scholarship over political correc...\n",
       "36    global warmist bill gate and his beach mansion...\n",
       "37    white house tell americans to endure high gas ...\n",
       "38    trump administration to launch black bean matt...\n",
       "39    it questionable whether joe biden understand a...\n",
       "40    belgium party leader islamic street thug have ...\n",
       "41    journalist name obstacle to peace between ukra...\n",
       "42    usda now ask people to register their vegetabl...\n",
       "43    this sentence explain what wrong with the webt...\n",
       "44    wake up even the mask make you sickmaybe the e...\n",
       "45    québec solidaire court big business ahead of p...\n",
       "46    lab origin theory confirm trump adviser debora...\n",
       "47    lie birx say she know covid vaccine would not ...\n",
       "48    ukraine global hotbe for mercenary and terrori...\n",
       "49    the great awakening continue ve vil not eet ze...\n",
       "50    west use ukraine as testing ground for its wea...\n",
       "51    liz truss elect uk 's new pm face tough job am...\n",
       "52    us debt surpass  trillionthe us national debt ...\n",
       "53    nesbit the we must fully embrace nuclear power...\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = test_df.preprocessed_headlines + test_df.preprocessed_articles\n",
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = roberta_encode(x_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 55s 22s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.3309119 , 0.33649427, 0.33259383],\n",
       "       [0.33235344, 0.3341213 , 0.3335252 ],\n",
       "       [0.33510315, 0.32963833, 0.3352585 ],\n",
       "       [0.33401918, 0.33048645, 0.3354944 ],\n",
       "       [0.33722687, 0.32900998, 0.33376315],\n",
       "       [0.32986614, 0.33815804, 0.33197582],\n",
       "       [0.3307315 , 0.33669588, 0.33257267],\n",
       "       [0.32957992, 0.33848011, 0.33194003],\n",
       "       [0.33234134, 0.3343677 , 0.33329096],\n",
       "       [0.33614987, 0.32889634, 0.3349538 ],\n",
       "       [0.33373123, 0.33119375, 0.33507496],\n",
       "       [0.3302326 , 0.337745  , 0.33202237],\n",
       "       [0.3299506 , 0.33806592, 0.33198354],\n",
       "       [0.33686548, 0.32934746, 0.3337871 ],\n",
       "       [0.33690703, 0.32874015, 0.3343528 ],\n",
       "       [0.33456612, 0.32907277, 0.33636117],\n",
       "       [0.3339    , 0.33140984, 0.33469012],\n",
       "       [0.33432826, 0.32954812, 0.3361236 ],\n",
       "       [0.33314273, 0.33312482, 0.3337325 ],\n",
       "       [0.33153775, 0.3353226 , 0.33313972],\n",
       "       [0.3287691 , 0.33932966, 0.33190125],\n",
       "       [0.33095533, 0.3364902 , 0.33255446],\n",
       "       [0.33743814, 0.32906902, 0.33349285],\n",
       "       [0.33761424, 0.3286424 , 0.33374333],\n",
       "       [0.33102813, 0.33688495, 0.33208692],\n",
       "       [0.3302651 , 0.3377456 , 0.33198932],\n",
       "       [0.3377183 , 0.32953805, 0.33274361],\n",
       "       [0.33364248, 0.33185935, 0.3344981 ],\n",
       "       [0.3369519 , 0.32931688, 0.33373114],\n",
       "       [0.33691776, 0.32829654, 0.33478567],\n",
       "       [0.3367536 , 0.3291939 , 0.33405247],\n",
       "       [0.3298439 , 0.33818063, 0.33197543],\n",
       "       [0.32983214, 0.33819842, 0.33196944],\n",
       "       [0.337596  , 0.32875317, 0.33365086],\n",
       "       [0.3307545 , 0.33697638, 0.3322691 ],\n",
       "       [0.33812904, 0.32808632, 0.33378464],\n",
       "       [0.33697295, 0.32928035, 0.3337467 ],\n",
       "       [0.3338904 , 0.33156046, 0.3345491 ],\n",
       "       [0.33324093, 0.3325622 , 0.33419687],\n",
       "       [0.33773854, 0.32850048, 0.33376095],\n",
       "       [0.3333218 , 0.33208045, 0.33459777],\n",
       "       [0.3297912 , 0.33824804, 0.33196083],\n",
       "       [0.33780065, 0.3284506 , 0.33374873],\n",
       "       [0.3378507 , 0.32895175, 0.33319753],\n",
       "       [0.33529097, 0.32940477, 0.3353042 ],\n",
       "       [0.33124533, 0.33603486, 0.33271974],\n",
       "       [0.33746174, 0.32830858, 0.33422968],\n",
       "       [0.3356065 , 0.32908103, 0.33531252],\n",
       "       [0.330622  , 0.33693972, 0.3324383 ],\n",
       "       [0.3378113 , 0.32929832, 0.33289045],\n",
       "       [0.3340944 , 0.33014756, 0.33575803],\n",
       "       [0.32986534, 0.33818   , 0.33195466],\n",
       "       [0.3309961 , 0.33678848, 0.3322154 ],\n",
       "       [0.33712253, 0.32857436, 0.3343031 ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = model_copy.predict(x_test)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 2, 2, 0, 1, 1, 1, 1, 0, 2, 1, 1, 0, 0, 2, 2, 2, 2, 1, 1, 1, 0, 0, 1, 1, 0, 2, 0, 0, 0, 1, 1, 0, 1, 0, 0, 2, 2, 0, 2, 1, 0, 0, 2, 1, 0, 0, 1, 0, 2, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "y_test_final = list()\n",
    "y_test_sub = list()\n",
    "for val in y_test:\n",
    "    if val[0]>val[1] and val[0]>val[2]:\n",
    "        y_test_final.append(0)\n",
    "        y_test_sub.append('opinion')\n",
    "    elif val[1]>val[0] and val[1]>val[2]:\n",
    "        y_test_final.append(1)\n",
    "        y_test_sub.append('reporting')\n",
    "    else:\n",
    "        y_test_final.append(2)\n",
    "        y_test_sub.append('satire')\n",
    "\n",
    "print(y_test_final) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_id = test_df.id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>article_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3121</th>\n",
       "      <td>reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3135</th>\n",
       "      <td>reporting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3134</th>\n",
       "      <td>satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3120</th>\n",
       "      <td>satire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3136</th>\n",
       "      <td>opinion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                class\n",
       "article_id           \n",
       "3121        reporting\n",
       "3135        reporting\n",
       "3134           satire\n",
       "3120           satire\n",
       "3136          opinion"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_test = {'article_id': y_test_id, 'class': y_test_sub} \n",
    "submit_test_df = pd.DataFrame(dict_test) \n",
    "submit_test_df.set_index('article_id', inplace=True)\n",
    "submit_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_test_df.to_csv('2_test_data_25_16.txt', sep='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sem_Eval-qQJYuRaW",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (v3.10.4:9d38120e33, Mar 23 2022, 17:29:05) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5bd67ae72067af90d04bfe162c4dbae485f2bc9cd35e8ebfff36ec4914b703f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
