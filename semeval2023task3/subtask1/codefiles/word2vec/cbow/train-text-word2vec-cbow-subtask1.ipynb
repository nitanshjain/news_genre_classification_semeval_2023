{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-05 01:17:03.069690: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "seed = 2000\n",
    "np.random.seed(seed)\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Flatten, Dense, Embedding\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, MaxPooling1D\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(433, 10)\n",
      "opinion      382\n",
      "reporting     41\n",
      "satire        10\n",
      "Name: genre, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_sub1_df = pd.read_csv('/Users/nitanshjain/Documents/Miscellaneous/SemEval/semeval2023task3/preprocessed_data/subtask1/train_subtask_1.csv')\n",
    "print(train_sub1_df.shape)\n",
    "train_sub1_df.head()\n",
    "print(train_sub1_df.genre.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    382\n",
      "1     41\n",
      "2     10\n",
      "Name: genre, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>genre</th>\n",
       "      <th>headlines</th>\n",
       "      <th>articles</th>\n",
       "      <th>preprocessed_headlines</th>\n",
       "      <th>pos_tags_headlines</th>\n",
       "      <th>er_tags_headlines</th>\n",
       "      <th>preprocessed_articles</th>\n",
       "      <th>pos_tags_articles</th>\n",
       "      <th>er_tags_articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>833042063</td>\n",
       "      <td>2</td>\n",
       "      <td>Chelsea Handler Admits She’s ‘Very Sexually At...</td>\n",
       "      <td>Far-left comedienne Chelsea Handler has admitt...</td>\n",
       "      <td>chelsea handler admit she s very sexually attr...</td>\n",
       "      <td>[(Chelsea, 'NNP'), (Handler, 'NNP'), (admit, '...</td>\n",
       "      <td>[(Chelsea Handler, 'PERSON', 380), (Robert Mue...</td>\n",
       "      <td>far leave comedienne chelsea handler have admi...</td>\n",
       "      <td>[(far, 'RB'), (leave, 'VB'), (comedienne, 'NNP...</td>\n",
       "      <td>[(Chelsea Handler, 'PERSON', 380), (FBI Specia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>832959523</td>\n",
       "      <td>2</td>\n",
       "      <td>How Theresa May Botched\\n</td>\n",
       "      <td>Those were the times…\\nThe Times page 1 is of ...</td>\n",
       "      <td>how theresa may botch</td>\n",
       "      <td>[(how, 'WRB'), (Theresa, 'NNP'), (May, 'NNP'),...</td>\n",
       "      <td>[(Theresa, 'GPE', 384), (May, 'DATE', 391)]</td>\n",
       "      <td>those be the time the times page 1 be of janua...</td>\n",
       "      <td>[(those, 'DT'), (be, 'VBP'), (the, 'DT'), (tim...</td>\n",
       "      <td>[(Times, 'ORG', 383), (1, 'CARDINAL', 397), (J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>833039623</td>\n",
       "      <td>2</td>\n",
       "      <td>Robert Mueller III Rests His Case—Dems NEVER W...</td>\n",
       "      <td>Carload of crazies headed to the White House w...</td>\n",
       "      <td>robert mueller iii rest his case dems never will</td>\n",
       "      <td>[(Robert, 'NNP'), (Mueller, 'NNP'), (III, 'NNP...</td>\n",
       "      <td>[(Robert Mueller III, 'PERSON', 380), (Dems, '...</td>\n",
       "      <td>carload of crazy head to the white house want ...</td>\n",
       "      <td>[(Carload, 'NNP'), (of, 'IN'), (crazy, 'JJ'), ...</td>\n",
       "      <td>[(the White House, 'ORG', 383), (Barack Obama,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>833032367</td>\n",
       "      <td>2</td>\n",
       "      <td>Robert Mueller Not Recommending Any More Indic...</td>\n",
       "      <td>But of course, this makes no difference to the...</td>\n",
       "      <td>robert mueller not recommend any more indictment</td>\n",
       "      <td>[(Robert, 'NNP'), (Mueller, 'NNP'), (not, 'RB'...</td>\n",
       "      <td>[(Robert Mueller, 'PERSON', 380)]</td>\n",
       "      <td>but of course this make no difference to the p...</td>\n",
       "      <td>[(but, 'CC'), (of, 'IN'), (course, 'NN'), (thi...</td>\n",
       "      <td>[(the New York Times, 'ORG', 383), (late Frida...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>814777937</td>\n",
       "      <td>2</td>\n",
       "      <td>The Far Right Is Trying to Co-opt the Yellow V...</td>\n",
       "      <td>This weekend in Manchester, England, Yellow Ve...</td>\n",
       "      <td>the far right be try to co opt the yellow vests</td>\n",
       "      <td>[(the, 'DT'), (Far, 'NNP'), (right, 'NN'), (be...</td>\n",
       "      <td>[]</td>\n",
       "      <td>this weekend in manchester england yellow vest...</td>\n",
       "      <td>[(this, 'DT'), (weekend, 'NN'), (in, 'IN'), (M...</td>\n",
       "      <td>[(this weekend, 'DATE', 391), (Manchester, 'GP...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  genre                                          headlines  \\\n",
       "0  833042063      2  Chelsea Handler Admits She’s ‘Very Sexually At...   \n",
       "1  832959523      2                          How Theresa May Botched\\n   \n",
       "2  833039623      2  Robert Mueller III Rests His Case—Dems NEVER W...   \n",
       "3  833032367      2  Robert Mueller Not Recommending Any More Indic...   \n",
       "4  814777937      2  The Far Right Is Trying to Co-opt the Yellow V...   \n",
       "\n",
       "                                            articles  \\\n",
       "0  Far-left comedienne Chelsea Handler has admitt...   \n",
       "1  Those were the times…\\nThe Times page 1 is of ...   \n",
       "2  Carload of crazies headed to the White House w...   \n",
       "3  But of course, this makes no difference to the...   \n",
       "4  This weekend in Manchester, England, Yellow Ve...   \n",
       "\n",
       "                              preprocessed_headlines  \\\n",
       "0  chelsea handler admit she s very sexually attr...   \n",
       "1                              how theresa may botch   \n",
       "2   robert mueller iii rest his case dems never will   \n",
       "3   robert mueller not recommend any more indictment   \n",
       "4    the far right be try to co opt the yellow vests   \n",
       "\n",
       "                                  pos_tags_headlines  \\\n",
       "0  [(Chelsea, 'NNP'), (Handler, 'NNP'), (admit, '...   \n",
       "1  [(how, 'WRB'), (Theresa, 'NNP'), (May, 'NNP'),...   \n",
       "2  [(Robert, 'NNP'), (Mueller, 'NNP'), (III, 'NNP...   \n",
       "3  [(Robert, 'NNP'), (Mueller, 'NNP'), (not, 'RB'...   \n",
       "4  [(the, 'DT'), (Far, 'NNP'), (right, 'NN'), (be...   \n",
       "\n",
       "                                   er_tags_headlines  \\\n",
       "0  [(Chelsea Handler, 'PERSON', 380), (Robert Mue...   \n",
       "1        [(Theresa, 'GPE', 384), (May, 'DATE', 391)]   \n",
       "2  [(Robert Mueller III, 'PERSON', 380), (Dems, '...   \n",
       "3                  [(Robert Mueller, 'PERSON', 380)]   \n",
       "4                                                 []   \n",
       "\n",
       "                               preprocessed_articles  \\\n",
       "0  far leave comedienne chelsea handler have admi...   \n",
       "1  those be the time the times page 1 be of janua...   \n",
       "2  carload of crazy head to the white house want ...   \n",
       "3  but of course this make no difference to the p...   \n",
       "4  this weekend in manchester england yellow vest...   \n",
       "\n",
       "                                   pos_tags_articles  \\\n",
       "0  [(far, 'RB'), (leave, 'VB'), (comedienne, 'NNP...   \n",
       "1  [(those, 'DT'), (be, 'VBP'), (the, 'DT'), (tim...   \n",
       "2  [(Carload, 'NNP'), (of, 'IN'), (crazy, 'JJ'), ...   \n",
       "3  [(but, 'CC'), (of, 'IN'), (course, 'NN'), (thi...   \n",
       "4  [(this, 'DT'), (weekend, 'NN'), (in, 'IN'), (M...   \n",
       "\n",
       "                                    er_tags_articles  \n",
       "0  [(Chelsea Handler, 'PERSON', 380), (FBI Specia...  \n",
       "1  [(Times, 'ORG', 383), (1, 'CARDINAL', 397), (J...  \n",
       "2  [(the White House, 'ORG', 383), (Barack Obama,...  \n",
       "3  [(the New York Times, 'ORG', 383), (late Frida...  \n",
       "4  [(this weekend, 'DATE', 391), (Manchester, 'GP...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "train_sub1_df['genre'] = le.fit_transform(train_sub1_df['genre'])\n",
    "print(train_sub1_df.genre.value_counts())\n",
    "train_sub1_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_tokens(df):\n",
    "    articles = df.preprocessed_articles\n",
    "    headlines = df.preprocessed_headlines\n",
    "    texts = articles + headlines\n",
    "    \n",
    "    tokens = list()\n",
    "    tokenizer = TweetTokenizer()\n",
    "    \n",
    "    for text in texts:\n",
    "        tokens.append(tokenizer.tokenize(text))\n",
    "    \n",
    "    df['tokens_text'] = tokens\n",
    "    \n",
    "    porter_stemmer = PorterStemmer()\n",
    "    # Get the stemmed_tokens\n",
    "    df['stemmed_tokens_text'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in df['tokens_text']]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sub1_df = creating_tokens(train_sub1_df)\n",
    "x_tokens = train_sub1_df.stemmed_tokens_text\n",
    "x_tokens = x_tokens.to_frame().reset_index()\n",
    "\n",
    "y = train_sub1_df.genre\n",
    "\n",
    "OUTPUT_FOLDER = '/Users/nitanshjain/Documents/Miscellaneous/SemEval/semeval2023task3/codefiles/subtask1/word2vec/cbow/text/'\n",
    "\n",
    "tokens = pd.Series(train_sub1_df.stemmed_tokens_text).values\n",
    "# print(tokens)\n",
    "word2vec_model_file = OUTPUT_FOLDER + 'word2vec_subtask1_' + str(200) + '.model'\n",
    "\n",
    "w2v_model = Word2Vec(tokens, min_count=1, vector_size=200, window=5, workers=4, sg=2)\n",
    "w2v_model.train(tokens, epochs=10, total_examples=len(tokens))\n",
    "w2v_model.save(word2vec_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(create_file, model_file, x):\n",
    "    sg_w2v_model = Word2Vec.load(model_file)\n",
    "    \n",
    "    with open(create_file, 'w+') as word2vec_file:\n",
    "        for index, row in x.iterrows():\n",
    "            model_vector = (np.mean([sg_w2v_model.wv[token] for token in row['stemmed_tokens_text']], axis=0)).tolist()\n",
    "            if index == 0:\n",
    "                header = \",\".join(str(ele) for ele in range(200))\n",
    "                word2vec_file.write(header)\n",
    "                word2vec_file.write(\"\\n\")\n",
    "            \n",
    "            if type(model_vector) is list:\n",
    "                line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
    "            else:\n",
    "                line1 = \",\".join([str(0) for i in range(200)])\n",
    "            word2vec_file.write(line1)\n",
    "            word2vec_file.write('\\n')\n",
    "    \n",
    "    df = pd.read_csv(create_file)\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(433, 200)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.057023</td>\n",
       "      <td>-0.046974</td>\n",
       "      <td>-0.119872</td>\n",
       "      <td>0.074033</td>\n",
       "      <td>0.093780</td>\n",
       "      <td>-0.163542</td>\n",
       "      <td>0.054604</td>\n",
       "      <td>0.352836</td>\n",
       "      <td>0.009015</td>\n",
       "      <td>0.010837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.211090</td>\n",
       "      <td>0.031598</td>\n",
       "      <td>-0.157998</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>-0.020315</td>\n",
       "      <td>0.083703</td>\n",
       "      <td>-0.062942</td>\n",
       "      <td>-0.089271</td>\n",
       "      <td>-0.019336</td>\n",
       "      <td>-0.022333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.073339</td>\n",
       "      <td>-0.025183</td>\n",
       "      <td>-0.065871</td>\n",
       "      <td>0.125752</td>\n",
       "      <td>0.101605</td>\n",
       "      <td>-0.101224</td>\n",
       "      <td>0.067087</td>\n",
       "      <td>0.359594</td>\n",
       "      <td>0.035522</td>\n",
       "      <td>0.031038</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147971</td>\n",
       "      <td>0.006097</td>\n",
       "      <td>-0.123449</td>\n",
       "      <td>-0.022419</td>\n",
       "      <td>-0.049047</td>\n",
       "      <td>0.088016</td>\n",
       "      <td>-0.073154</td>\n",
       "      <td>-0.062970</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>-0.094848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.066100</td>\n",
       "      <td>-0.016196</td>\n",
       "      <td>-0.102681</td>\n",
       "      <td>0.112423</td>\n",
       "      <td>0.107274</td>\n",
       "      <td>-0.121176</td>\n",
       "      <td>0.055603</td>\n",
       "      <td>0.321244</td>\n",
       "      <td>0.019246</td>\n",
       "      <td>0.030151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130723</td>\n",
       "      <td>0.035938</td>\n",
       "      <td>-0.119866</td>\n",
       "      <td>-0.009457</td>\n",
       "      <td>-0.027275</td>\n",
       "      <td>0.080994</td>\n",
       "      <td>-0.098678</td>\n",
       "      <td>-0.070525</td>\n",
       "      <td>-0.040590</td>\n",
       "      <td>-0.040808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.117753</td>\n",
       "      <td>-0.041735</td>\n",
       "      <td>-0.092932</td>\n",
       "      <td>0.142670</td>\n",
       "      <td>0.078955</td>\n",
       "      <td>-0.188116</td>\n",
       "      <td>-0.010440</td>\n",
       "      <td>0.377951</td>\n",
       "      <td>0.029416</td>\n",
       "      <td>-0.020622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145707</td>\n",
       "      <td>-0.009353</td>\n",
       "      <td>-0.132705</td>\n",
       "      <td>-0.005330</td>\n",
       "      <td>-0.069417</td>\n",
       "      <td>0.072274</td>\n",
       "      <td>-0.082185</td>\n",
       "      <td>-0.076866</td>\n",
       "      <td>-0.029585</td>\n",
       "      <td>-0.020619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.106022</td>\n",
       "      <td>0.006376</td>\n",
       "      <td>-0.010921</td>\n",
       "      <td>0.079795</td>\n",
       "      <td>0.062115</td>\n",
       "      <td>-0.141638</td>\n",
       "      <td>0.036544</td>\n",
       "      <td>0.301768</td>\n",
       "      <td>-0.060693</td>\n",
       "      <td>0.068646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.099761</td>\n",
       "      <td>0.048389</td>\n",
       "      <td>-0.096497</td>\n",
       "      <td>-0.101092</td>\n",
       "      <td>0.004428</td>\n",
       "      <td>0.025751</td>\n",
       "      <td>-0.113758</td>\n",
       "      <td>-0.060745</td>\n",
       "      <td>-0.052044</td>\n",
       "      <td>-0.072730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.057023 -0.046974 -0.119872  0.074033  0.093780 -0.163542  0.054604   \n",
       "1  0.073339 -0.025183 -0.065871  0.125752  0.101605 -0.101224  0.067087   \n",
       "2  0.066100 -0.016196 -0.102681  0.112423  0.107274 -0.121176  0.055603   \n",
       "3  0.117753 -0.041735 -0.092932  0.142670  0.078955 -0.188116 -0.010440   \n",
       "4  0.106022  0.006376 -0.010921  0.079795  0.062115 -0.141638  0.036544   \n",
       "\n",
       "          7         8         9  ...       190       191       192       193  \\\n",
       "0  0.352836  0.009015  0.010837  ...  0.211090  0.031598 -0.157998  0.000963   \n",
       "1  0.359594  0.035522  0.031038  ...  0.147971  0.006097 -0.123449 -0.022419   \n",
       "2  0.321244  0.019246  0.030151  ...  0.130723  0.035938 -0.119866 -0.009457   \n",
       "3  0.377951  0.029416 -0.020622  ...  0.145707 -0.009353 -0.132705 -0.005330   \n",
       "4  0.301768 -0.060693  0.068646  ...  0.099761  0.048389 -0.096497 -0.101092   \n",
       "\n",
       "        194       195       196       197       198       199  \n",
       "0 -0.020315  0.083703 -0.062942 -0.089271 -0.019336 -0.022333  \n",
       "1 -0.049047  0.088016 -0.073154 -0.062970  0.001119 -0.094848  \n",
       "2 -0.027275  0.080994 -0.098678 -0.070525 -0.040590 -0.040808  \n",
       "3 -0.069417  0.072274 -0.082185 -0.076866 -0.029585 -0.020619  \n",
       "4  0.004428  0.025751 -0.113758 -0.060745 -0.052044 -0.072730  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_train_filename = OUTPUT_FOLDER + 'word2vec_subtask1_train_' + str(200) + '.csv'\n",
    "word2vec_train_df = create_file(word2vec_train_filename, word2vec_model_file, x_tokens)\n",
    "print(word2vec_train_df.shape)\n",
    "word2vec_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.422275</td>\n",
       "      <td>0.443241</td>\n",
       "      <td>0.143030</td>\n",
       "      <td>0.265252</td>\n",
       "      <td>0.583837</td>\n",
       "      <td>0.552153</td>\n",
       "      <td>0.654039</td>\n",
       "      <td>0.609157</td>\n",
       "      <td>0.440488</td>\n",
       "      <td>0.432760</td>\n",
       "      <td>...</td>\n",
       "      <td>0.939529</td>\n",
       "      <td>0.585157</td>\n",
       "      <td>0.191353</td>\n",
       "      <td>0.761807</td>\n",
       "      <td>0.523067</td>\n",
       "      <td>0.671572</td>\n",
       "      <td>0.725099</td>\n",
       "      <td>0.438314</td>\n",
       "      <td>0.516057</td>\n",
       "      <td>0.635049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.505764</td>\n",
       "      <td>0.577355</td>\n",
       "      <td>0.345767</td>\n",
       "      <td>0.578049</td>\n",
       "      <td>0.625683</td>\n",
       "      <td>0.791262</td>\n",
       "      <td>0.706424</td>\n",
       "      <td>0.644080</td>\n",
       "      <td>0.562323</td>\n",
       "      <td>0.513028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579259</td>\n",
       "      <td>0.431321</td>\n",
       "      <td>0.363042</td>\n",
       "      <td>0.657239</td>\n",
       "      <td>0.411289</td>\n",
       "      <td>0.689004</td>\n",
       "      <td>0.663997</td>\n",
       "      <td>0.632746</td>\n",
       "      <td>0.653904</td>\n",
       "      <td>0.235455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.468722</td>\n",
       "      <td>0.632662</td>\n",
       "      <td>0.207572</td>\n",
       "      <td>0.497436</td>\n",
       "      <td>0.655996</td>\n",
       "      <td>0.714711</td>\n",
       "      <td>0.658230</td>\n",
       "      <td>0.445878</td>\n",
       "      <td>0.487514</td>\n",
       "      <td>0.509504</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480809</td>\n",
       "      <td>0.611343</td>\n",
       "      <td>0.380845</td>\n",
       "      <td>0.715209</td>\n",
       "      <td>0.495990</td>\n",
       "      <td>0.660623</td>\n",
       "      <td>0.511274</td>\n",
       "      <td>0.576900</td>\n",
       "      <td>0.372825</td>\n",
       "      <td>0.533243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.733031</td>\n",
       "      <td>0.475485</td>\n",
       "      <td>0.244171</td>\n",
       "      <td>0.680367</td>\n",
       "      <td>0.504566</td>\n",
       "      <td>0.457867</td>\n",
       "      <td>0.381070</td>\n",
       "      <td>0.738956</td>\n",
       "      <td>0.534257</td>\n",
       "      <td>0.307756</td>\n",
       "      <td>...</td>\n",
       "      <td>0.566334</td>\n",
       "      <td>0.338118</td>\n",
       "      <td>0.317042</td>\n",
       "      <td>0.733664</td>\n",
       "      <td>0.332038</td>\n",
       "      <td>0.625381</td>\n",
       "      <td>0.609959</td>\n",
       "      <td>0.530017</td>\n",
       "      <td>0.446988</td>\n",
       "      <td>0.644494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.673003</td>\n",
       "      <td>0.771579</td>\n",
       "      <td>0.552064</td>\n",
       "      <td>0.300101</td>\n",
       "      <td>0.414522</td>\n",
       "      <td>0.636197</td>\n",
       "      <td>0.578244</td>\n",
       "      <td>0.345223</td>\n",
       "      <td>0.120088</td>\n",
       "      <td>0.662468</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304081</td>\n",
       "      <td>0.686455</td>\n",
       "      <td>0.496977</td>\n",
       "      <td>0.305403</td>\n",
       "      <td>0.619326</td>\n",
       "      <td>0.437362</td>\n",
       "      <td>0.421042</td>\n",
       "      <td>0.649195</td>\n",
       "      <td>0.295636</td>\n",
       "      <td>0.357337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.422275  0.443241  0.143030  0.265252  0.583837  0.552153  0.654039   \n",
       "1  0.505764  0.577355  0.345767  0.578049  0.625683  0.791262  0.706424   \n",
       "2  0.468722  0.632662  0.207572  0.497436  0.655996  0.714711  0.658230   \n",
       "3  0.733031  0.475485  0.244171  0.680367  0.504566  0.457867  0.381070   \n",
       "4  0.673003  0.771579  0.552064  0.300101  0.414522  0.636197  0.578244   \n",
       "\n",
       "        7         8         9    ...       190       191       192       193  \\\n",
       "0  0.609157  0.440488  0.432760  ...  0.939529  0.585157  0.191353  0.761807   \n",
       "1  0.644080  0.562323  0.513028  ...  0.579259  0.431321  0.363042  0.657239   \n",
       "2  0.445878  0.487514  0.509504  ...  0.480809  0.611343  0.380845  0.715209   \n",
       "3  0.738956  0.534257  0.307756  ...  0.566334  0.338118  0.317042  0.733664   \n",
       "4  0.345223  0.120088  0.662468  ...  0.304081  0.686455  0.496977  0.305403   \n",
       "\n",
       "        194       195       196       197       198       199  \n",
       "0  0.523067  0.671572  0.725099  0.438314  0.516057  0.635049  \n",
       "1  0.411289  0.689004  0.663997  0.632746  0.653904  0.235455  \n",
       "2  0.495990  0.660623  0.511274  0.576900  0.372825  0.533243  \n",
       "3  0.332038  0.625381  0.609959  0.530017  0.446988  0.644494  \n",
       "4  0.619326  0.437362  0.421042  0.649195  0.295636  0.357337  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm = MinMaxScaler()\n",
    "\n",
    "x_scaled = pd.DataFrame(mm.fit_transform(word2vec_train_df))\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=seed)\n",
    "x_scaled.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy Score  0.8899224806201552\n",
      "Best Parameters {'n_neighbors': 7, 'weights': 'uniform'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_params = {\n",
    "    'n_neighbors':range(1,15),\n",
    "    'weights':['uniform', 'distance']\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "clf = GridSearchCV(knn, knn_params, cv=cv)\n",
    "clf.fit(x_scaled, y)\n",
    "\n",
    "print('Best Accuracy Score ', clf.best_score_)\n",
    "print('Best Parameters', clf.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy Score  0.8853241719520792\n",
      "Best Parameters {'C': 0.21544346900318834, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_params = {\n",
    "    'C':np.logspace(-2,2,7), \n",
    "    'penalty':['l1', 'l2', 'elasticnet', None]\n",
    "}\n",
    "\n",
    "lr = LogisticRegression()\n",
    "clf = GridSearchCV(lr, lr_params, cv=cv)\n",
    "clf.fit(x_scaled, y)\n",
    "\n",
    "print('Best Accuracy Score ', clf.best_score_)\n",
    "print('Best Parameters', clf.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decison Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy Score  0.8883544749823821\n",
      "Best Parameters {'criterion': 'entropy', 'max_depth': 2, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_params = {\n",
    "    'max_depth':range(2,15),\n",
    "    'criterion':['gini', 'entropy', 'log_loss'],\n",
    "    'splitter':['best', 'random']\n",
    "}\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "clf = GridSearchCV(dt, dt_params, cv=cv)\n",
    "clf.fit(x_scaled, y)\n",
    "\n",
    "print('Best Accuracy Score ', clf.best_score_)\n",
    "print('Best Parameters', clf.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy Score  0.890697674418605\n",
      "Best Parameters {'criterion': 'gini', 'max_depth': 4}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfc_params = {\n",
    "    'max_depth':range(3,15),\n",
    "    'criterion':['gini', 'entropy', 'log_loss'],\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "clf = GridSearchCV(rfc, rfc_params, cv=cv)\n",
    "clf.fit(x_scaled, y)\n",
    "\n",
    "print('Best Accuracy Score ', clf.best_score_)\n",
    "print('Best Parameters', clf.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy Score 0.8845265588914549\n",
      "CPU times: user 10.6 ms, sys: 8.98 ms, total: 19.6 ms\n",
      "Wall time: 20.2 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(x_scaled, y)\n",
    "\n",
    "print('Train Accuracy Score', mnb.score(x_scaled, y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy Score  0.8853241719520791\n",
      "Best Parameters {'degree': 2, 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_params = {\n",
    "    'kernel':['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'degree':range(2,5)\n",
    "}\n",
    "\n",
    "svc = SVC()\n",
    "clf = GridSearchCV(svc, svc_params, cv=cv)\n",
    "clf.fit(x_scaled, y)\n",
    "\n",
    "print('Best Accuracy Score ', clf.best_score_)\n",
    "print('Best Parameters', clf.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada-Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy Score 0.7413394919168591\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada = AdaBoostClassifier()\n",
    "ada.fit(x_scaled, y)\n",
    "\n",
    "print('Train Accuracy Score', ada.score(x_scaled, y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = x_scaled[:,:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-27 14:13:43.797353: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 11720, 64)         16896     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 11720, 64)         0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 11720, 32)         12416     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 11720, 32)         0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 11720, 16)         3136      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 11720, 16)         0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 11720, 8)          800       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 11720, 8)          0         \n",
      "                                                                 \n",
      " output (LSTM)               (None, 11720, 1)          40        \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 11720)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 11721     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,009\n",
      "Trainable params: 45,009\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model_lstm = Sequential()\n",
    "\n",
    "model_lstm.add(LSTM(64, input_shape = x_scaled.shape[1:], return_sequences = True))\n",
    "model_lstm.add(MaxPooling1D(pool_size = (5), padding = 'same'))\n",
    "model_lstm.add(Dropout(rate=0.8))\n",
    "\n",
    "model_lstm.add(LSTM(32, return_sequences=True))\n",
    "model_lstm.add(MaxPooling1D(pool_size = (5), padding = 'same'))\n",
    "model_lstm.add(Dropout(rate=0.5))\n",
    "\n",
    "model_lstm.add(LSTM(16, return_sequences=True))\n",
    "model_lstm.add(MaxPooling1D(pool_size = (5), padding = 'same'))\n",
    "model_lstm.add(Dropout(rate=0.5))\n",
    "\n",
    "model_lstm.add(LSTM(8, return_sequences=True))\n",
    "model_lstm.add(MaxPooling1D(pool_size = (5), padding = 'same'))\n",
    "model_lstm.add(Dropout(rate=0.5))\n",
    "\n",
    "model_lstm.add(LSTM(1, return_sequences=True, name='output'))\n",
    "model_lstm.add(MaxPooling1D(pool_size = (5), padding = 'same'))\n",
    "model_lstm.add(Dropout(rate=0.5))\n",
    "\n",
    "model_lstm.add(Dense(64, activation = 'relu'))\n",
    "model_lstm.add(Flatten())\n",
    "model_lstm.add(Dense(1, activation='softmax'))\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "4/7 [================>.............] - ETA: 2:36 - loss: 0.6169 - accuracy: 0.0859"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "model_lstm.fit(x_scaled, y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=10,\n",
    "            shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model_clstm  =  Sequential()\n",
    "\n",
    "model_clstm.add(Conv1D(32, (3), padding = 'same', activation = 'relu', input_shape = x_scaled.shape[1:]))\n",
    "model_clstm.add(MaxPooling1D(pool_size = (5), padding = 'same'))\n",
    "model_clstm.add(Dropout(rate = 0.8))\n",
    "\n",
    "model_clstm.add(Conv1D(16, (3), padding = 'same', activation = 'relu', input_shape = x_scaled.shape[1:]))\n",
    "model_clstm.add(MaxPooling1D(pool_size = (5), padding = 'same'))\n",
    "model_clstm.add(Dropout(rate = 0.8))\n",
    "\n",
    "model_clstm.add(Conv1D(8, (3), padding = 'same', activation = 'relu', input_shape = x_scaled.shape[1:]))\n",
    "model_clstm.add(MaxPooling1D(pool_size = (5), padding = 'same'))\n",
    "model_clstm.add(Dropout(rate = 0.8))\n",
    "\n",
    "model_clstm.add(LSTM(32, return_sequences = True))\n",
    "model_clstm.add(MaxPooling1D(pool_size = (5), padding = 'same'))\n",
    "model_clstm.add(Dropout(rate = 0.5))\n",
    "\n",
    "model_clstm.add(LSTM(16, return_sequences = True))\n",
    "model_clstm.add(MaxPooling1D(pool_size = (5), padding = 'same'))\n",
    "model_clstm.add(Dropout(rate = 0.5))\n",
    "\n",
    "model_clstm.add(LSTM(1, return_sequences = True))\n",
    "model_clstm.add(MaxPooling1D(pool_size = (5), padding = 'same'))\n",
    "model_clstm.add(Dropout(rate = 0.5))\n",
    "\n",
    "model_clstm.add(Dense(64, activation = 'relu'))\n",
    "model_clstm.add(Flatten())\n",
    "model_clstm.add(Dense(1, activation = 'softmax'))\n",
    "\n",
    "model_clstm.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model_clstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "model_clstm.fit(x_scaled, y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=10,\n",
    "            shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.layers import LSTM, Embedding, Bidirectional\n",
    "\n",
    "\n",
    "model_bilstm  =  Sequential()\n",
    "\n",
    "model_bilstm.add(Bidirectional(LSTM(32, input_shape = x_scaled.shape[1:], return_sequences=True)))\n",
    "model_bilstm.add(MaxPooling1D(pool_size = (5), padding = 'same'))\n",
    "model_bilstm.add(Dropout(rate = 0.5))\n",
    "\n",
    "model_bilstm.add(Bidirectional(LSTM(16, return_sequences=True)))\n",
    "model_bilstm.add(MaxPooling1D(pool_size = (5), padding = 'same'))\n",
    "model_bilstm.add(Dropout(rate = 0.5))\n",
    "\n",
    "model_bilstm.add(Bidirectional(LSTM(1, return_sequences=True)))\n",
    "model_bilstm.add(MaxPooling1D(pool_size = (5), padding = 'same'))\n",
    "model_bilstm.add(Dropout(rate = 0.5))\n",
    "\n",
    "model_bilstm.add(Dense(64, activation = 'relu'))\n",
    "model_bilstm.add(Flatten())\n",
    "model_bilstm.add(Dense(1, activation = 'softmax'))\n",
    "\n",
    "model_bilstm.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model_bilstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "model_bilstm.fit(x_scaled, y,\n",
    "            batch_size=batch_size,\n",
    "            epochs=10,\n",
    "            shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (v3.10.4:9d38120e33, Mar 23 2022, 17:29:05) [Clang 13.0.0 (clang-1300.0.29.30)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
